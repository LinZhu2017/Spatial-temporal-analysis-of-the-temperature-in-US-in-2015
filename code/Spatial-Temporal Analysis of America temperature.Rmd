---
title: "Spatial-Temporal Analysis of America temperature in 2015"
author: "Lin Zhu"
output: 
  html_document:
      toc: true
      toc_depth: 3
      theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
The film "the wandering earth", which has attracted a lot of attention recently, sets up an extremely harsh climate which is close to collapse after a great change of climate. Such a catactrophic scenario is not just a figment of the bad weather that is common in many places. Climate change and potential global warming have become the significant issue of the day, monitoring temperatures across the region, therefore, is becoming increasingly important in response to weather-related disasters.

The data files US_weather_2015 and weather_stations correspondingly contain temperature of the United States at different stations and and their locations (marked by longitude, latitude and elevation) from January 1 to December 31, 2015, a period of 365 days. Given spatial statistics models generally only take longitude and latitude into consideration, we substract the variable elevation (elev) and only take the temperature of one elevation into consideration in the same time at the same location (including stn, lon and lat). Furthermore, to facilitate analysis, we should merge two datasets according to the same variable station number (stn). Top of all, we assume that temporal resolution is one day, and hence we are expected to divide the whole data into 365 sub dataframes including the information of each day, and denote them by "day1", "day2" and so on.


# Exploratory data analysis
## Data description
The following packages are used in this project. 

```{r eval=T, echo=T,warning=FALSE, include=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(plotly)
library(maps)
library(mapdata) 
library(maptools)
library(graphics)
library(gstat)
library(sp)
library(viridis)
library(fields)
library(gridExtra)
library(xts) 
library(spacetime) 
```

Read two data files US_weather_2015 and weather_stations

```{r}
load("weather_data.RData")
load("weather_stations.RData")
```

Dataframe weather has five variables: stn, year, month, day and temp.
```{r}
head(weather_data)
```

Dataframe loc has seven variables: stn, name, country, state, lat, lon and elev.
```{r echo=TRUE}
head(weather_stations)
```

Merge weather and loc by common variables stn, lat and lon and remove duplicate rows based on all variables.
```{R eval=FALSE, include=TRUE}
## Extract the states in the main US territory
states = unique(statesMap$region)
weather_stations_sub = filter(weather_stations, lat < 50 & lat > 25 & lon > (-125) & lon < (-65))
## merge weather and location(loc)
library(dplyr)
weather_data2 = filter(weather_data, stn %in% weather_stations_sub$stn)
matchPos = match(weather_data2$stn, weather_stations_sub$stn)
weather_data2 = cbind(weather_data2, weather_stations_sub[matchPos, c("lon", "lat","elev")])

weather_data2 = na.omit(weather_data2)
## Remove Duplicate Rows based on all variables
weather_data2 = distinct(weather_data2)
## add a variable "num" to identify date
weather_data2 = mutate(weather_data2, num = as.numeric(ISOdate(2015,month,day)-ISOdate(2015,1,1),
                                                       units="days")+1)
weather_data2 = mutate(weather_data2, date = as.Date(paste(year,month,day,sep = "-")))

## Sort data by date, lon and lat
weather_data2 = arrange(weather_data2, num, lon, lat)
weather_data2 = mutate(weather_data2, lon_rank = min_rank(lon), lat_rank = min_rank(lat))
```

```{r include=FALSE}
load("weather_data2.RData")
```


```{r}
head(weather_data2)
```


## Monthly data 
At first, we should compute the mean of the temperature by month so that we can obtain a rough idea of the monthly temperature in the United States from the histogram and boxplot.

```{r eval=FALSE, include=T}
## Mean vs. Month:
month_mean=weather_data2 %>%
  group_by(month)%>%
  summarise(temp_month = mean(temp, na.rm=TRUE))
```

```{r, echo=F, eval=T}
load("month_mean.RData")
```

```{r warning=F}
## Histogram of Average monthly temperature 
fit <- fitted(loess(month_mean$temp_month ~ month_mean$month))
p_hist <- plot_ly(data = month_mean, x = ~month, y = ~temp_month, type = "bar", showlegend=FALSE,
                  marker=list(color=~month, showscale=FALSE)) %>%
  add_lines(y = fit, showlegend=FALSE, color = 'black') %>%
  layout(title = 'Average monthly temperature in the US in 2015',
         showlegend=FALSE, xaxis = list(side="right", showgrid=FALSE),
         yaxis=list(title = "temp (Fahrenheit)",showgrid=FALSE))
p_hist

## Boxplot of US monthly temperature
p_box <- ggplot(weather_data2, aes(month, temp, group = month, color = month, fill = month)) + 
  geom_boxplot(outlier.shape = NA) + 
  scale_fill_gradientn(colours = terrain.colors((12))) + 
  theme(legend.position='none') +
  ggtitle("Boxplot of US monthly temperature")

p_box
```

In view of the histogram of average monthly data, it can be illustrated that the monthly average temperature, at first, shows a trend of rising and reaches its maximum in July with approximately 75 Fahrenheit, and then gradually falls as winter coming after October. While the boxplot displays the distribution of the temperature in distinct stations during each month. In accordance with the histogram, the median of the temperature in July reaches the peak. Furthermore, the data in July concentrates in the smallest range compared to the rest of months. In addition, the temperature of different locations vary widely in February, which demonstrates, in the coldest season, the temperature is mostlly influenced by longitude and latitude coordinates. On the contrary, as for hot seasons, there is no significant effect on temperature from coordinates. The above two pictures both reflect the trend of temperature over time.

## Data of Jan 1, 2015
To familiarize ourselves with the geography of the dataset, we will initially ignore the temporal component of the dataset and examine the spatial distribution of temperatures on a single day. Take day 1 as an example, therefore, we extract data of January 1, 2015.

```{R echo=T, eval=F}
## Get the data on Jan 1st, 2015
day1 = filter(weather_data2, num==1) 
```

### Temperature distribution image of Jan 1, 2015
Since the temperature points are discrete with respect to the whole space (in the longitude-latitude-axis matrix, only 0.02% of elements are not NA), it is not available to show the United States temperatures by using the image.plot command in the fields package. In this case, we introduce the following method to plot the discrete points' temperature distribution image.

```{r echo=F, eval=T}
load("day1.RData")
```

```{r}
day1_= day1
day1_$hover <- with(day1_, paste("<br>", "Lon:", lon,  
                                 "<br>","Lat:", lat, 
                                 "<br>","Elev:", elev,
                                 "<br>", "Temp:", temp))
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showlakes = TRUE,
  lakecolor = toRGB('white'))

p1 <- plot_geo(day1_, lat = ~lat, lon = ~lon) %>%
  add_markers(
    text = ~hover, showlegend=FALSE,
    marker=list(color = ~temp, showscale=FALSE),
    hoverinfo = "text") %>%
  layout(title = 'US temp (Fahrenheit) on Jan 1st, 2015',
         geo = g, showlegend=T)
library(plotly)
ggplotly(p1)
```

From the above discrete points' temperature distribution image, it seems that it was deep winter across large parts of the United States on January 1st, 2015. Northern and northwestern parts have very cold temperatures, especially in New England, the Midwest, and the Mid-Atlantic states. Though temperatures were typically milder here than in the North and Midwest, the weather in states in the Southeast and Southwest were still cold. A pronounced temperature gradient is visible from highs of over 85.6 Fahrenheit in the north of the study area which is near to the equator to a low of -14.7 Fahrenheit towards the southern boundary. This is not only indicative of spatial correlation in the dataset, but it also shows that the data are not stationary, as the mean temperature must vary strongly with latitude. With the exception of a few regions along the western coastline of the United States, temperature decreased with latitude being higher. And there could be a nonlinear relationship between temperature and longitude. Apart from longitude and latitude coordinates, temperatures usually were connected to elevation, with stations at higher elevations being colder. Next, to explore the trend of the temperature affected by various factors, we can draw images of temperature changing with longitude, latitude and altitude respectively. 

### Mean and variance of US temperature by latitude on Jan 1 in 2015
```{r}
library(graphics)
## Mean vs. Latitude:
lat_mean=day1 %>%
  group_by(lat)%>%
  summarise(temp_lat = mean(temp, na.rm=TRUE)) 

f1 = ggplot(data = lat_mean) + 
  geom_point(mapping = aes(x = lat, y = temp_lat))+
  geom_smooth(mapping = aes(x = lat, y = temp_lat)) +
  labs(x = "lat",y = "temp", title = 'Mean temp by Latitude')

## Variance vs. Latitude:
lat_var=day1 %>%
  group_by(lat)%>%
  summarise(temp_var_lat = var(temp, na.rm=TRUE))
f2 = ggplot(data = lat_var) + 
  geom_point(mapping = aes(x = lat, y = temp_var_lat))+
  geom_smooth(mapping = aes(x = lat, y = temp_var_lat))+
  labs(x = "lat",y = "variance of temp", title = 'Variance in temp by Latitude')
```

```{r echo=T, eval=T, warning=F}
grid.arrange(f1, f2, ncol=2)
```

### Mean and variance of US temperature by longitude on Jan 1 in 2015
```{r}
## Mean vs. Longitude:
lon_mean=day1 %>%
  group_by(lon)%>%
  summarise(temp_lon = mean(temp, na.rm=TRUE)) 
f3 = ggplot(data = lon_mean) + 
  geom_point(mapping = aes(x = lon, y = temp_lon))+
  geom_smooth(mapping = aes(x = lon, y = temp_lon))+
  labs(x = "lon",y = "temp", title = 'Mean temp by Longitude')

## Variance vs. Longitude:
lon_var=day1 %>%
  group_by(lon)%>%
  summarise(temp_var_lon = var(temp, na.rm=TRUE))
f4 = ggplot(data = lon_var) + 
  geom_point(mapping = aes(x = lon, y = temp_var_lon))+
  geom_smooth(mapping = aes(x = lon, y = temp_var_lon))+
  labs(x = "lon",y = "variance of temp", title = 'Variance in temp by Longitude')
```

```{r echo=T, eval=T, warning=F}
grid.arrange(f3, f4, ncol=2)
```


### Mean and variance of US temperature by elevation on Jan 1 in 2015
```{r}
## Mean vs. Elevation:
elev_mean=day1 %>%
  group_by(elev)%>%
  summarise(temp_elev = mean(temp, na.rm=TRUE)) 
f5 = ggplot(data = elev_mean) + 
  geom_point(mapping = aes(x = elev, y = temp_elev))+
  geom_smooth(mapping = aes(x = elev, y = temp_elev))+
  labs(x = "elev",y = "temp", title = 'Mean temp by Elevation')

## Variance vs. Elevation:
elev_var=day1 %>%
  group_by(elev)%>%
  summarise(temp_var_elev = var(temp, na.rm=TRUE))
f6 = ggplot(data = elev_var) + 
  geom_point(mapping = aes(x = elev, y = temp_var_elev))+
  geom_smooth(mapping = aes(x = elev, y = temp_var_elev))+
  labs(x = "elev",y = "variance of temp", title = 'Variance in temp by Elevation')
```

```{r echo=T, eval=T, warning=F}
grid.arrange(f5, f6, ncol=2)
```

These three figures illustrate the non-stationarity of the temperature, so it is necessary to figure out the temperature trend before selecting a feasible covariance model. As latitude is away from the equator, the mean temperature falls. The fitted mean and variance curves across longitudes further demonstrate the nonlinear relationship between temperature and longitude. Maybe affected by a small number of extreme values, the variance in certain location is extraordinarily large. Perhaps the figures of a certain day cannot provide sufficient information for the trend. Therefore, we draw the corresponding mean(variance) fitted curves between the temperature and some explanatory variables (including longitude and latitude) of 12 months (see the animation). It can be found the figures of each month display a similar pattern that the temperature decreases linearly with latitude, while the relationship between temperature and longitude is non-linear. In this case, it makes sense to use the same form of a mean function to predict the temperature of over the grid on each day, based on the observations.

# Research objectives
In light to images of temperature changing with longitude, latitude, altitude and month, we can confirm that there are some trends in location and time marginally. For simplicity, in variogram modeling and kriging throughout this paper, we will treat the latitude and longitude coordinates as if they are Cartesian. In geostatistics, we generally consider the temperature is normally distributed with mean and covariance structure being functions of latitude, longitude and time. Furthermore, the mean function may be the linear combination or other nonlinear forms of three factors. Apart from this, the covariance may introduce the $Mat\acute{e}rn$ class, which is popular in spatial statistics. Considering these cases, we will figure out the structure of the normal distribution by the following steps.
1. Construct spatial data analysis regardless of the influence of time and find the most appropriate model.
2. Construct Spatial-Temporal analysis.
3. Various forms of Kriging can be used to attempt to fill gaps so that we can obtain the complete temperature variation diagram across the United States. 

# Spatial analysis of the temperature

## The spatial model
In the function "spatialProcess" of the package "fields", it assumes a spatial model
$$Y_k=P(x_k)+Z(x_k)\beta+g(x_k)+e_k$$
The estimated surface is the best linear unbiased estimate (BLUE) of the first three terms given observed data.

- **Notation**

    - $Y_k$: dependent variable (i.e. temperature) at location $x_k$.

    - $P(\cdot)$: a low degree polynomial (default is a linear function, we can adjust the order of polynomial by mkrig.args).

    - $Z(\cdot)$: a matrix of covariates in a linear model (the elevation can be put in this part).

    - $g(\cdot)$: a mean zero Gaussian process with covariance function $K$ defined by the marginal process variance $\rho$ and the range parameter $\theta$, which means $K(\rho,\theta) = \rho\cdot \text{Cov}(\theta)$.

    - $e_k$: independent normal error, $e_k \sim N(0,\sigma^2)$, where $\sigma^2$ is called the nugget variance.

## Define train data and test data
Longitude, latitude, altitude and corresponding temperature were extracted from the data "day1", 80% of which was training set and the rest was test set.

```{r echo=F, eval=T}
load("X_new.RData")
load("Y_new.RData")
load("Z_new.RData")
```

```{r echo=TRUE, eval=F}
X_new = day1[,6:7]
Y_new = day1[5]
Z_new = day1[,8]
```

```{r}
set.seed(123)
index = sample(nrow(Y_new),nrow(Y_new)*0.8)
train_X = X_new[index,]
test_X = X_new[-index,]
train_y = as.matrix(Y_new)[index]
test_y = as.matrix(Y_new)[-index]
train_Z = Z_new[index]
test_Z = Z_new[-index]
```

## Model fitting 

### Model 1: Assume quadratic polynomial trend in P
```{r echo=T, eval=F}
library(fields)
par_est = spatialProcess(train_X,train_y,Z=train_Z,
                         mKrig.args = list(m = 3),
                         Distance = "rdist.earth",
                         cov.args = list(Covariance = "Matern",
                                         smoothness = 1))
# where m=3 means quadratic form
```

```{r echo=F, eval=T}
load("par_est.RData")
```


```{r}
print(par_est)
sigma.square = as.numeric(par_est$sigma.MLE)^2  # nugget variance (sigma^2)
rho = as.numeric(par_est$rho.MLE)               # process variance (rho) 
theta = as.numeric(par_est$theta.MLE)        # range parameter(theta) (miles)
```


### Model 2: Assume linear polynomial trend in P
```{r echo=T, eval=F}
par_est2 = spatialProcess(train_X,train_y,Z=train_Z,
                         mKrig.args = list(m = 2),
                         Distance = "rdist.earth",
                         cov.args = list(Covariance = "Matern",
                                         smoothness = 1))
# where m=2 means linear form
```

```{r echo=F, eval=T}
load("par_est2.RData")
```

```{r}
print(par_est2)
sigma.square = as.numeric(par_est$sigma.MLE)^2  # nugget variance (sigma^2)
rho = as.numeric(par_est$rho.MLE)               # process variance (rho) 
theta = as.numeric(par_est$theta.MLE)       # range parameter (theta) (miles)
```

## Model evaluation

```{r}
## The maximum distance among all pairs 
out=rdist.earth(cbind(train_X$lon,train_X$lat),miles = T)
range(out)[2]   
```
Considering the maximum distance among all pairs of train data, the "par_est" and "par_est2" given by two models seem to be feasible.


   - **d**: These are coefficients of the polynomial fixed part and the        covariates Z, the first 6 numbers are coefficients of polynomial part, while the last term is the coefficient of the extra covariate Z. 

   - **eff.df**: It is estimate of effective degrees of freedom which has one to one mapping with $\lambda=\sigma^2/\rho$. If the eff.df is very small, this suggests that the surface is well represented by a low order polynomial. Otherwise, if the eff.df approaches to the line number of train data, then the surface is close to interpolating the observations and suggests a small or aero value for the nugget variance.

   - **lnProfileLike**: log Profile likelihood for lambda.

   - **RMSE**: relative mean square prediction error.


### As for quadratic polynomial trend

```{r}
par_est$d
```
The coefficient of Z is so small that it may be ignored.

```{r}
par_est$eff.df
```

```{r}
par_est$lnProfileLike
```

```{r}
pre = predict(par_est, xnew = test_X, Z = test_Z)
RMSE = mean((pre-test_y)^2)/median(test_y)
RMSE
```

```{r}
set.panel(2,2)
plot(par_est)  
```


### As for linear polynomial trend

```{r}
par_est2$d
```
The coefficient of Z is so small that it may be ignored.

```{r}
par_est2$eff.df
```

```{r}
par_est2$lnProfileLike
```

```{r}
pre2 = predict(par_est2, xnew = test_X, Z = test_Z)
RMSE2 = mean((pre2-test_y)^2)/median(test_y)
RMSE2
```

```{r}
set.panel(2,2)
plot(par_est2)  
```

From above output, we can know that the log Profile likelihood for lambda of model 1 is larger than model 2, and this implies the good fitness of the former. However, the behavior of RMSE in model 2 is smaller than that in model 1. In view of goodness of fit and interpretability with relatively large RMSE, we still choose model 1. In fact, the covariance model also can be selected, but the prediction effect of candidate models have no obvious difference, so the $Mat\acute{e}rn$ model with flexible parameters is used to fit the model of the whole data set.

## Redefine the train data 

```{r, echo=TRUE, eval=F}
train_X = X_new
train_y = Y_new
train_Z = Z_new
```

```{r echo=F, eval=T}
load("par_est_whole.RData")
```

```{r, echo=TRUE, eval=F}
## parameter estimation
#(fields use great circle distances, approxiamte the earth by a ball)
par_est_whole = spatialProcess(train_X,train_y,Z=train_Z,
                               mKrig.args = list(m = 3),
                               Distance = "rdist.earth",
                               cov.args = list(Covariance = "Matern",smoothness = 1))
# where m=3 means quadratic form
print(par_est_whole)
sigma.square = as.numeric(par_est_whole$sigma.MLE)^2     # nugget variance (sigma^2)  4.225
rho = as.numeric(par_est_whole$rho.MLE)                  # process variance (rho)   60.12
theta = as.numeric(par_est_whole$theta.MLE)              # range parameter (theta)  150.646(miles) 
```


## Construct the prediction region
```{r echo=T, eval=F}
## Create testing data by meshgrid  resolution of (60/50)бу x (25/50)бу of longitude and latitude
lon_num = 50
lat_num = 50
lon_seq = seq(-125,-65,length.out=lon_num)
lat_seq = seq(25,50,length.out=lat_num)

## Restrict the grid to the USA mainland
require(mapdata) 
tmp <- map('worldHires', 'Usa', fill=TRUE, plot=FALSE) 
require(maptools) 
US.boundary <- map2SpatialPolygons(tmp, IDs = tmp$names, proj4string = CRS(as.character(NA))) 
US.bbox.grid <- SpatialPoints(cbind(rep(lon_seq,length(lat_seq)), 
                                    rep(lat_seq,each=length(lon_seq)))) 
gridded(US.bbox.grid) <- TRUE
US.grid <- US.bbox.grid[!is.na(over(US.bbox.grid, US.boundary)),] 
Z = as.matrix(rep(mean(Z_new),length(US.grid@coords[,1])))
pre_whole = predict(par_est_whole, xnew = US.grid@coords,Z = Z) # Z is not necessary
```

## Plot of krigged temperature

```{r echo=F, eval=T}
load("pre_whole.RData")
load("US.grid.RData")
```

```{r }
df=as.data.frame(US.grid)
df$z <- pre_whole
colnames(df)=c("lon","lat","temp")

library(ggplot2)
statesMap <- map_data("state")

library(viridis)
ggplot(df, aes(lon, lat,fill = temp)) + 
  geom_raster(interpolate = F) + 
  scale_fill_continuous(type="viridis",alpha=0.9) +
  geom_polygon(data = statesMap, aes(x=long, y = lat, group = group), color = "black",fill="grey", alpha=0) +
  labs(x = "lon",y = "lat", title = 'Krigged US temp (Fahrenheit) on Jan 1st, 2015') + 
  theme(plot.title = element_text(hjust = 0.4))
```


## spatial pattern of the residuals after fitting
```{r}
library(plotly)
testdata = cbind(X_new,Z_new,par_est_whole$residuals)
colnames(testdata)=c("lon","lat", "elev", "temp")

testdata$hover <- with(testdata, paste("<br>", "Lon:", lon,  
                                       "<br>","Lat:", lat, 
                                       "<br>","Elev:", elev,
                                       "<br>", "ResTemp:", temp))
bubble = plot_ly(testdata, x = ~lon, y = ~lat) %>%
  add_markers(
    text = ~hover, showlegend=FALSE,
    marker=list(color = ~temp, 
                size = ~2*abs(temp), opacity = 0.5, 
                type = 'scatter', mode = 'markers', showscale=FALSE),
    hoverinfo = "text") %>%
  layout(title = 'Residual from two order polynomial trend with Matern class covariance',
         xaxis = list(showgrid = FALSE),
         yaxis = list(showgrid = FALSE))
```

```{r }
bubble
```

```{r}
range(par_est_whole$residuals)
```

The range of the fitted residuals is $(-13.14837,18.24184)$, and the bubble diagram reflects the overall fitted effect of the spatial model with the quadratic polynomial trend and $Mat\acute{e}rn$ covariance, except that a few coordinates have a large deviation in the predicted value which could be a result of high elevation. 

# Spatial-temporal analysis of the temperature
Although the model above is adept at extracting spatial information, temperature is also affected by the seasons, so it makes sense to take time into account. Furthermore, it should be noted that since the temperature data does not follow the normal distribution, we should compute the spatial trend of the temperature rather than use the temperature directly. The following code simultaneously computes the residual and the temperature prediction of gridded points for each day during the peorid of the first 10 days in 2015. 

### Construct spatial(temporal) object and class dataframe

```{r echo=T, eval=F}
## Unique longitude and latitude coordinates of all data
lon.lat.1to10 <- weather_data3[,c(1,6,7)]
lon.lat.1to10.uni <- distinct(lon.lat.1to10,lon,lat,.keep_all = T)
```

```{r echo=T, eval=F}
### spatial object
sp.1to10 = lon.lat.1to10.uni[,c(2,3)]
row.names(sp.1to10) = lon.lat.1to10.uni$stn
coordinates(sp.1to10)=~lon+lat
library(sp)
sp.1to10 = SpatialPoints(sp.1to10,
                         proj4string=CRS("+init=epsg:4326 +proj=longlat +ellps=WGS84 
                                         +datum=WGS84 +no_defs +towgs84=0,0,0"))
```

```{r echo=T, eval=F}
### temporal object
time = as.Date("2015-01-01")+0:9
```

```{r echo=T, eval=F}
### object of class data.frame
library(dplyr)
temp.1to10 <- NULL
for(i in 1:10){
  day <- filter(weather_data3, num==i)
  dat <- day[,c(8,9,6,7,5)]
  out <- distinct(dat,lon,lat,.keep_all = T)
  tmp = left_join(lon.lat.1to10.uni,out,by = c("lon","lat"))
  tmp = arrange(tmp,lon,lat)
  temp.1to10 = rbind(temp.1to10, tmp)
  
}
temp.1to10 = temp.1to10[,c(1,4,5,2,3,6)]
temp.data = data.frame(temp.1to10$temp)
colnames(temp.data)="temp"
```

## Generate STFDF object
The STFDF object is constructed to facilitate the further spatial-temporal analysis. Since it is not possible to allocate a large amount of space in R, and the time dependence of temperature is short, we only trandform the first 10 days data into a STFDF object (a class for spatial-temporal data with full space-time grid). 
```{r echo=T, eval=F}
## generate STFDF
library(spacetime)
sp.1to10.time = STFDF(sp.1to10, time, temp.data, endTime = delta(time))
na.stations <- which(apply(as(sp.1to10.time, "xts"), 2, function(x) sum(is.na(x))>0))
sp.1to10.time = sp.1to10.time[-na.stations,]
```

```{r}
load("sp.1to10.time.RData")
```

```{r}
## summarize the structure of sp.1to10.time
summary(sp.1to10.time)
```

## Calculate spatial-temporal sample variogram
```{r echo=T, eval=F}
## Investigating spatio-temporal structure
vst <- variogramST(temp ~ polym(lon, lat, degree=2, raw=TRUE), sp.1to10.time)
vst2 <- variogramST(temp ~ polym(lon, lat, degree=2, raw=TRUE), sp.1to10.time, tlags=0:4)
vst3 <- variogramST(temp ~ polym(lon, lat, degree=3, raw=TRUE), sp.1to10.time, tlags=0:4)
vst4 <- variogramST(temp ~ polym(lon, lat, degree=1, raw=TRUE), sp.1to10.time, tlags=0:4)
```

```{r echo=F, eval=T}
load("vst.RData")
load("vst2.RData")
load("vst3.RData")
load("vst4.RData")
```

```{r}
head(vst)
```


```{r}
summary(vst)
```


- **Notation**

    - **timelag**: the time lag used
    
    - **spacelag**: the midpoint in the spatial lag intervals as passed by the parameter boundaries
    
    - **avgDist**: the average distance between the point pairs found in a distance interval over all temporal lags (i.e. the averages of the values dist per temporal lag).


```{r}
vstp1 = plot(vst, xlab="separation (km)", ylab="separation (+days)", main="(a) Semivariance, degree=2, Temp")
vstp2 = plot(vst2, xlab="separation (km)", ylab="separation (+days)", main="(b) Semivariance, degree=2, Temp")
vstp3 = plot(vst3, xlab="separation (km)", ylab="separation (+days)", main="(c) Semivariance, degree=3, Temp")
vstp4 = plot(vst4, xlab="separation (km)", ylab="separation (+days)", main="(d) Semivariance, degree=1, Temp")
```

```{r}
grid.arrange(vstp1, vstp2, vstp3, vstp4, ncol=2)
```


```{r}
vstl1 = plot(vst, map = FALSE, xlab="separation (km)", ylab = "Semivariance, Temp", main="(a) degree=2")
vstl2 = plot(vst2, map = FALSE, xlab="separation (km)", ylab = "Semivariance, Temp", main="(b) degree=2")
vstl3 = plot(vst3, map = FALSE, xlab="separation (km)", ylab = "Semivariance, Temp", main="(c) degree=3")
vstl4 = plot(vst4, map = FALSE, xlab="separation (km)", ylab = "Semivariance, Temp", main="(d) degree=1")
```

```{r}
grid.arrange(vstl1, vstl2, vstl3, vstl4, ncol=2)
```

Since we have only 10 days of data, the model vst which has ten time lags seems not appropriate (see figure (a)). While model vst2 and vst3 have a better behavior as a result of the semivariogram of the two models rapidly converge to the maximum. It should be noted that these figures plot the semivariogram of residuals after removing trend (formulized by "formula"). Therefore, except a very short spatial interval, the semivariogram is pretty large, and the covariance is 0, indicating that points beyond a certain spatial lag have no correlation and the spatial trend is extracted well. For interpretability, we choose model vst2.

## Construct a spatio-temporal variogram of a given type
We consider the separable covariance model and then get the fitted variogram from model vst2. The separable model has separate spatial and temporal structures, which are considered to interact only multiplicatively and so can be ???t independently but with a common sill. The covariance structure is 
$$C(h,u)=C_s(h)\cdot C_t(u)$$
where $s$ and $t$ are space and time index respectively, and $h$ and $u$ are the space lag and time lag.

```{r echo=T, eval=F}
(estimated.sill <- quantile(na.omit(vst2$gamma), .8))
(vgm.sep <- vgmST(stModel="separable", space=vgm(0.9,"Exp", 300,0.1), 
                  time=vgm(0.95,"Exp", 2, 0.05), sill=estimated.sill))
vgmf.sep <- fit.StVariogram(vst2, vgm.sep, method="L-BFGS-B", 
                            lower=c(100,0.001,1,0.001,40), 
                            control=list(maxit=500)) 
```

```{r}
load("vgmf.sep.RData")
```


```{r}
attr(vgmf.sep, "optim.output")$par
```

The result means that points beyond about 210 km have no significant relationship, and the temperature is hardly influenced by that after 7days. The nugget effects in time and space are both small, which guarantee a smoother temperature surface.

```{r}
attr(vgmf.sep, "optim.output")$value 
```

The goodness-of-???t is expressed by the error sum of squares in the value attribute of the ???tted model.

```{r}
plot(vst, vgmf.sep) 
```

```{r}
plot(vst, vgmf.sep, map=FALSE)
```

```{r echo=F, eval=T}
load("US.grid.RData")
```



```{r echo=T, eval=F}
colnames(US.grid@coords) = c("lon","lat")
US.grid <- STF(sp=as(US.grid,"SpatialPoints"), time=sp.1to10.time@time)
proj4string(US.grid)=proj4string(sp.1to10.time)

k.de.sep <- krigeST(temp~polym(lon, lat, degree=2, raw=TRUE),
                    data=sp.1to10.time, newdata=US.grid, modelList=vgmf.sep) 
gridded(k.de.sep@sp) <- TRUE
```

```{r}
load("k.de.sep.RData")
```


```{r}
plot.zlim <- seq(floor(min(k.de.sep$var1.pred)), 
                 ceiling(max(k.de.sep$var1.pred)), by = 0.5)
stplot(k.de.sep, main="The temperature of the first 10 days in 2015", 
       sub="Separable space-time model", 
       col.regions=bpy.colors(length(plot.zlim)), at=plot.zlim) 
```

```{r echo=T, eval=F}
day1.sp = SpatialPoints(cbind(day1$lon,day1$lat))
colnames(day1.sp@coords) = c("lon","lat")
day1.sp <- STF(sp=as(day1.sp,"SpatialPoints"), time=sp.1to10.time@time)
proj4string(day1.sp)=proj4string(sp.1to10.time)

k.de.sep.day1.fit <- krigeST(temp~polym(lon, lat, degree=2, raw=TRUE),
                    data=sp.1to10.time, newdata=day1.sp, modelList=vgmf.sep) 
gridded(k.de.sep.day1.fit@sp) <- TRUE
save(k.de.sep.day1.fit,file="k.de.sep.day1.fit")
```

```{r echo=F, eval=T}
load("k.de.sep.day1.fit.RData")
```

```{r}
day1.fit = k.de.sep.day1.fit[,"2015-01-01"]$var1.pred
day1.res = day1.fit - day1$temp
```

## spatial pattern of the residuals after fitting
```{r}
library(plotly)
testdata2 = cbind(X_new,Z_new,day1.res)
colnames(testdata2)=c("lon","lat", "elev", "temp")

testdata2$hover <- with(testdata2, paste("<br>", "Lon:", lon,  
                                       "<br>","Lat:", lat, 
                                       "<br>","Elev:", elev,
                                       "<br>", "ResTemp:", temp))
bubble2 = plot_ly(testdata2, x = ~lon, y = ~lat) %>%
  add_markers(
    text = ~hover, showlegend=FALSE,
    marker=list(color = ~temp, 
                size = ~0.3*abs(temp), opacity = 0.5, 
                type = 'scatter', mode = 'markers', showscale=FALSE),
    hoverinfo = "text") %>%
  layout(title = 'Residual from spatial-temporal model with separable covariance',
         xaxis = list(showgrid = FALSE),
         yaxis = list(showgrid = FALSE))
```

```{r }
bubble2
```

It can be concluded from the residual diagram after fitting that the spatial-temporal model is not very suitable, since the fitting temperature at high latitudes is higher than the true value while for low latitudes, the contrary is the case. Maybe the hypothesis of separability is not correct, therefore, in the following study, we need to explore a more reasonable assumption of covariance structure.